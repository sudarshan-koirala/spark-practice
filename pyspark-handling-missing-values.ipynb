{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ed1880-3d25-4a56-b5e0-bcc0df18ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ce838d-589a-401b-a4c5-fce75138855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e101ce73-d188-403d-91f0-3dce3ef4d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start session\n",
    "spark = SparkSession.builder.appName(\"Learning-spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69073023-2362-4f7e-962f-f84d9d17a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df2088f2-b7da-45f8-bf9b-e0d28ccac445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the dataset\n",
    "df_pyspark = spark.read.csv(\"datasets/tips.csv\", header=True, inferSchema=True)\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7a9620-7dde-4aa2-a7f5-e4900225ddff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('total_bill', 'double'),\n",
       " ('tip', 'double'),\n",
       " ('sex', 'string'),\n",
       " ('smoker', 'string'),\n",
       " ('day', 'string'),\n",
       " ('time', 'string'),\n",
       " ('size', 'int')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtypes of dataframe\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b553ceb3-5fa1-4c16-93df-d05635a2c2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- total_bill: double (nullable = true)\n",
      " |-- tip: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# similar to dtypes, see the printSchema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1958e5ca-6a23-454c-9e3d-29b5524a1a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+----+\n",
      "|total_bill| tip|   sex|smoker|day|size|\n",
      "+----------+----+------+------+---+----+\n",
      "|     16.99|1.01|Female|    No|Sun|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|   3|\n",
      "+----------+----+------+------+---+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the columns\n",
    "df_pyspark.drop(\"time\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df7139d-32ef-4245-8865-b62f0cea61f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a52353-00b9-4306-a365-62a3e145d099",
   "metadata": {},
   "source": [
    "As we see, pyspark dataframe doesn't have inplace so quick data manipulation or test can be done as we go. But, if you need that manipulation to exist, need to create new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f921d1e6-c3bc-48aa-bffe-2ca9c3d6efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset with  missing values\n",
    "df_pyspark = spark.read.csv(\"datasets/class-grades.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1234071d-3c2c-4097-9202-014777949ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+-------+--------+-----+\n",
      "|Prefix|Assignment|Tutorial|Midterm|TakeHome|Final|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "|     5|     57.14|   34.09|  64.38|   51.48| 52.5|\n",
      "|     8|     95.05|  105.49|   67.5|   99.07|68.33|\n",
      "|     8|      83.7|   83.17|   30.0|   63.15|48.89|\n",
      "|     7|     81.22|   96.06|  49.38|  105.93|80.56|\n",
      "|     8|     91.32|   93.64|   95.0|  107.41|73.89|\n",
      "|     7|      95.0|   92.58|  93.12|   97.78|68.06|\n",
      "|     8|     95.05|  102.99|  56.25|   99.07| 50.0|\n",
      "|     7|     72.85|   86.85|   60.0|    null|56.11|\n",
      "|     8|     84.26|    93.1|   47.5|   18.52|50.83|\n",
      "|     7|      90.1|   97.55|  51.25|   88.89|63.61|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aabf6e0-5725-4cf7-b1db-ee32b381adfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+-------+--------+-----+\n",
      "|Prefix|Assignment|Tutorial|Midterm|TakeHome|Final|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "|     5|     57.14|   34.09|  64.38|   51.48| 52.5|\n",
      "|     8|     95.05|  105.49|   67.5|   99.07|68.33|\n",
      "|     8|      83.7|   83.17|   30.0|   63.15|48.89|\n",
      "|     7|     81.22|   96.06|  49.38|  105.93|80.56|\n",
      "|     8|     91.32|   93.64|   95.0|  107.41|73.89|\n",
      "|     7|      95.0|   92.58|  93.12|   97.78|68.06|\n",
      "|     8|     95.05|  102.99|  56.25|   99.07| 50.0|\n",
      "|     8|     84.26|    93.1|   47.5|   18.52|50.83|\n",
      "|     7|      90.1|   97.55|  51.25|   88.89|63.61|\n",
      "|     7|     80.44|    90.2|   75.0|   91.48|39.72|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop na\n",
    "df_pyspark.na.drop().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fe21229-e628-41b0-8414-42c4f1e32a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+-------+--------+-----+\n",
      "|Prefix|Assignment|Tutorial|Midterm|TakeHome|Final|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "|     5|     57.14|   34.09|  64.38|   51.48| 52.5|\n",
      "|     8|     95.05|  105.49|   67.5|   99.07|68.33|\n",
      "|     8|      83.7|   83.17|   30.0|   63.15|48.89|\n",
      "|     7|     81.22|   96.06|  49.38|  105.93|80.56|\n",
      "|     8|     91.32|   93.64|   95.0|  107.41|73.89|\n",
      "|     7|      95.0|   92.58|  93.12|   97.78|68.06|\n",
      "|     8|     95.05|  102.99|  56.25|   99.07| 50.0|\n",
      "|     8|     84.26|    93.1|   47.5|   18.52|50.83|\n",
      "|     7|      90.1|   97.55|  51.25|   88.89|63.61|\n",
      "|     7|     80.44|    90.2|   75.0|   91.48|39.72|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# there are different ways of using na.drop (any = how -> default one)\n",
    "df_pyspark.na.drop(how=\"any\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c391a755-bc86-4eb2-88d6-9903938bc38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+-------+--------+-----+\n",
      "|Prefix|Assignment|Tutorial|Midterm|TakeHome|Final|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "|     5|     57.14|   34.09|  64.38|   51.48| 52.5|\n",
      "|     8|     95.05|  105.49|   67.5|   99.07|68.33|\n",
      "|     8|      83.7|   83.17|   30.0|   63.15|48.89|\n",
      "|     7|     81.22|   96.06|  49.38|  105.93|80.56|\n",
      "|     8|     91.32|   93.64|   95.0|  107.41|73.89|\n",
      "|     7|      95.0|   92.58|  93.12|   97.78|68.06|\n",
      "|     8|     95.05|  102.99|  56.25|   99.07| 50.0|\n",
      "|     7|     72.85|   86.85|   60.0|    null|56.11|\n",
      "|     8|     84.26|    93.1|   47.5|   18.52|50.83|\n",
      "|     7|      90.1|   97.55|  51.25|   88.89|63.61|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can provide threshold\n",
    "# If specified, drop rows that have less than `thresh` non-null values.This overwrites the `how` parameter. \n",
    "df_pyspark.na.drop(how=\"any\", thresh=3).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd47b11-91ee-46c8-afc1-19fd11ac61f7",
   "metadata": {},
   "source": [
    "Here we have just one null values so nothing removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6a493a7-5a8f-460e-8c81-f8d0050599ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+-------+--------+-----+\n",
      "|Prefix|Assignment|Tutorial|Midterm|TakeHome|Final|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "|     5|     57.14|   34.09|  64.38|   51.48| 52.5|\n",
      "|     8|     95.05|  105.49|   67.5|   99.07|68.33|\n",
      "|     8|      83.7|   83.17|   30.0|   63.15|48.89|\n",
      "|     7|     81.22|   96.06|  49.38|  105.93|80.56|\n",
      "|     8|     91.32|   93.64|   95.0|  107.41|73.89|\n",
      "|     7|      95.0|   92.58|  93.12|   97.78|68.06|\n",
      "|     8|     95.05|  102.99|  56.25|   99.07| 50.0|\n",
      "|     8|     84.26|    93.1|   47.5|   18.52|50.83|\n",
      "|     7|      90.1|   97.55|  51.25|   88.89|63.61|\n",
      "|     7|     80.44|    90.2|   75.0|   91.48|39.72|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can specify which columns to look through subset parameter.\n",
    "df_pyspark.na.drop(how=\"any\", subset=[\"TakeHome\"]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f82df-17f0-410e-a001-6713e6a2f42a",
   "metadata": {},
   "source": [
    "There is null value at TakeHome column which got removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af07acb0-7d35-4808-9ed5-566ce0bdf9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+-------+--------+-----+\n",
      "|Prefix|Assignment|Tutorial|Midterm|TakeHome|Final|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "|     5|     57.14|   34.09|  64.38|   51.48| 52.5|\n",
      "|     8|     95.05|  105.49|   67.5|   99.07|68.33|\n",
      "|     8|      83.7|   83.17|   30.0|   63.15|48.89|\n",
      "|     7|     81.22|   96.06|  49.38|  105.93|80.56|\n",
      "|     8|     91.32|   93.64|   95.0|  107.41|73.89|\n",
      "|     7|      95.0|   92.58|  93.12|   97.78|68.06|\n",
      "|     8|     95.05|  102.99|  56.25|   99.07| 50.0|\n",
      "|     7|     72.85|   86.85|   60.0|    null|56.11|\n",
      "|     8|     84.26|    93.1|   47.5|   18.52|50.83|\n",
      "|     7|      90.1|   97.55|  51.25|   88.89|63.61|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71c56913-24b1-47b2-aefb-285388a85f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+-------+--------+-----+\n",
      "|Prefix|Assignment|Tutorial|Midterm|TakeHome|Final|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "|     5|     57.14|   34.09|  64.38|   51.48| 52.5|\n",
      "|     8|     95.05|  105.49|   67.5|   99.07|68.33|\n",
      "|     8|      83.7|   83.17|   30.0|   63.15|48.89|\n",
      "|     7|     81.22|   96.06|  49.38|  105.93|80.56|\n",
      "|     8|     91.32|   93.64|   95.0|  107.41|73.89|\n",
      "|     7|      95.0|   92.58|  93.12|   97.78|68.06|\n",
      "|     8|     95.05|  102.99|  56.25|   99.07| 50.0|\n",
      "|     7|     72.85|   86.85|   60.0|    50.0|56.11|\n",
      "|     8|     84.26|    93.1|   47.5|   18.52|50.83|\n",
      "|     7|      90.1|   97.55|  51.25|   88.89|63.61|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filling the missing values with 50\n",
    "df_pyspark.na.fill(50).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66caf05b-fac4-4c1a-95c8-20ecc7136113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use Imputer to impute missing values\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d26f2115-8564-418f-ac41-c5fd4be99580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets impute with median\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"TakeHome\", \"Midterm\"],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in [\"TakeHome\", \"Midterm\"]],\n",
    ").setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "077d8d90-8422-4a43-8d37-8a29dd2ea509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+-------+--------+-----+\n",
      "|Prefix|Assignment|Tutorial|Midterm|TakeHome|Final|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "|     5|     57.14|   34.09|  64.38|   51.48| 52.5|\n",
      "|     8|     95.05|  105.49|   67.5|   99.07|68.33|\n",
      "|     8|      83.7|   83.17|   30.0|   63.15|48.89|\n",
      "|     7|     81.22|   96.06|  49.38|  105.93|80.56|\n",
      "|     8|     91.32|   93.64|   95.0|  107.41|73.89|\n",
      "|     7|      95.0|   92.58|  93.12|   97.78|68.06|\n",
      "|     8|     95.05|  102.99|  56.25|   99.07| 50.0|\n",
      "|     7|     72.85|   86.85|   60.0|    null|56.11|\n",
      "|     8|     84.26|    93.1|   47.5|   18.52|50.83|\n",
      "|     7|      90.1|   97.55|  51.25|   88.89|63.61|\n",
      "|     7|     80.44|    90.2|   75.0|   91.48|39.72|\n",
      "|     6|     86.26|    80.6|  74.38|   87.59| 77.5|\n",
      "|     8|     97.16|  103.71|   72.5|   93.52|63.33|\n",
      "|     7|     91.28|   83.53|  81.25|   99.81|92.22|\n",
      "|     8|      84.8|   89.08|  44.38|   16.91|35.83|\n",
      "|     7|     93.83|   95.43|  88.12|   80.93| 90.0|\n",
      "|     8|      84.8|   89.08|   47.5|   16.91|53.33|\n",
      "|     4|     92.01|  102.52|  38.75|   86.11|49.17|\n",
      "|     8|     55.14|   81.85|   75.0|   56.11| 62.5|\n",
      "|     8|     93.04|   82.93|  79.38|   83.33|91.11|\n",
      "+------+----------+--------+-------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5dd6cf7-21a0-419b-ac34-a60d71dcc7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+-------+--------+-----+----------------+---------------+\n",
      "|Prefix|Assignment|Tutorial|Midterm|TakeHome|Final|TakeHome_imputed|Midterm_imputed|\n",
      "+------+----------+--------+-------+--------+-----+----------------+---------------+\n",
      "|     5|     57.14|   34.09|  64.38|   51.48| 52.5|           51.48|          64.38|\n",
      "|     8|     95.05|  105.49|   67.5|   99.07|68.33|           99.07|           67.5|\n",
      "|     8|      83.7|   83.17|   30.0|   63.15|48.89|           63.15|           30.0|\n",
      "|     7|     81.22|   96.06|  49.38|  105.93|80.56|          105.93|          49.38|\n",
      "|     8|     91.32|   93.64|   95.0|  107.41|73.89|          107.41|           95.0|\n",
      "|     7|      95.0|   92.58|  93.12|   97.78|68.06|           97.78|          93.12|\n",
      "|     8|     95.05|  102.99|  56.25|   99.07| 50.0|           99.07|          56.25|\n",
      "|     7|     72.85|   86.85|   60.0|    null|56.11|           87.96|           60.0|\n",
      "|     8|     84.26|    93.1|   47.5|   18.52|50.83|           18.52|           47.5|\n",
      "|     7|      90.1|   97.55|  51.25|   88.89|63.61|           88.89|          51.25|\n",
      "|     7|     80.44|    90.2|   75.0|   91.48|39.72|           91.48|           75.0|\n",
      "|     6|     86.26|    80.6|  74.38|   87.59| 77.5|           87.59|          74.38|\n",
      "|     8|     97.16|  103.71|   72.5|   93.52|63.33|           93.52|           72.5|\n",
      "|     7|     91.28|   83.53|  81.25|   99.81|92.22|           99.81|          81.25|\n",
      "|     8|      84.8|   89.08|  44.38|   16.91|35.83|           16.91|          44.38|\n",
      "|     7|     93.83|   95.43|  88.12|   80.93| 90.0|           80.93|          88.12|\n",
      "|     8|      84.8|   89.08|   47.5|   16.91|53.33|           16.91|           47.5|\n",
      "|     4|     92.01|  102.52|  38.75|   86.11|49.17|           86.11|          38.75|\n",
      "|     8|     55.14|   81.85|   75.0|   56.11| 62.5|           56.11|           75.0|\n",
      "|     8|     93.04|   82.93|  79.38|   83.33|91.11|           83.33|          79.38|\n",
      "+------+----------+--------+-------+--------+-----+----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputed columns alongside the original df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958414d-7070-49f5-a267-4375481d79f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
